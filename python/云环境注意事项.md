### Google colab tensorflow 版本切换

*因该可以适用于所有切换版本的库*
tensorflow包有1.x和2.x两个版本，colab默认2.x版本，若要使用1.x版本则用下列code，让后重启runtime，使其生效，即”重启运行并重新运行所有单元格“

```shell
%tensorflow_version 1.x # 该代码必须加到import 该库之前
```

### AWS Sage Maker Studio Lab

* import cv2报错
  ImportError: libgthread-2.0.so.0: cannot open shared
  
  ```python
  ! pip install opencv-contrib-python
  ! conda install glib=2.51.0 -y 
  ```
  
  [参考链接](https://blog.csdn.net/m0_47136227/article/details/122333468)
  *注：在云服务器中一般都不能切换到root环境，所以apt-get也无法使用，坑人的是这个环境也无法使用vim。所以只能在conda或者pip的包着对应依赖的替代包（一般都能找到）[非root用户安装包](https://blog.csdn.net/qq_41314786/article/details/114273847)*
  
  * SL在线处理视频卡顿现象
    由于是在云端进行处理，在通过网络传输过来，存在延迟所以卡顿
    解决方案：考虑不显示视频，而是将视频处理完后，下载下来观看
  * 

### Ali TianChi

### 恒源云使用

为什么使用恒源云，有比较多的代金券和机器选择

* 代码我通过pycharm进行连接同步，本地文件单向映射到服务器文件，没次更改一个服务器文件就**需要重新配置**，配置成功可通过本地电脑进行操作
* 默认使用阿里云，速度比清华源慢
  
  ~~~
  
  pip install -i https://pypi.tuna.tsinghua.edu.cn/simple gevent #这样就会从清华这边的镜像去安装gevent库
  
  ~

#### 数据存储

* hy-tmp是其高速读写硬盘，每建立一个实例，该实例就有一个ht-tmp有免费50GB，超过收费，超过24小时不启动制动销毁。

* 个人数据，所有实例通用，目前不知道个人硬盘如何使用，不免费，收费，长期存储一些数据。可以通过oss快速上传，也可以通过页面上传。根目录oss://，可以用于快速保存结果和加载代码文件到服务器中。

* 本地个人电脑磁盘，主要可以同步代码文件。

* 公共网盘，如百度网盘和阿里云盘，这里建议使用阿里云盘，速度非常快，可以用于下载数据集，上传个人数据。
  
  这些存储都可以在服务器上进行交互，值得注意的是，**训练完的结果（训练参数、模型文件、代码等不包括训练数据的文件）记的保存到其他地方，不要保存到服务器上，以免被销毁**

#### tinyYolov3+Alphapose+StGcn

效果不好，yolo能够精准的检测到人，但alphapose的检测有点不好，另外stgcn还需要训练

#### mmaction2在环境中使用

版本对齐：

mmaction2

mmcv-full, 1.7.0

mmpose

mmdet

numpy 1.23.0才不会报那个numpy.core的错，报这个错就是numpy版本太低

scipy

是预测整个图形的，要把它的单独的框拿出来，对人物进行标注，这里可以仿照上面

### tf1

#### 静态图机制

每一个有运算，函数产生的tensor本身都是一个计算图，只需要将sess.run(xx, dict{})，dict，该计算图的输入带入进去，就可以得到相应的结果xx，不需要考虑中间数据
问题：如果我要用中间数据怎么办
解决：分开写sess.run，分成多个计算图，把中间数据断开

#### tensorboard细节

**细节复杂建议多看api和例子**

#### 矩阵乘法

* 点/数乘 tf.multiply()
* 矩阵乘法 tf.mutmal()

#### 卷积层细节

strides参数[]
ksize参数[n, x, y, c] n为batch数，(x,y)为一个卷积核的大小，c为卷积核的“个数”
一般卷积核，[x, y, ic, oc]，(x,y) 为卷积核大小，ic为输入卷积核的channel数，oc为输出卷积核的个数，如果输入灰度图ic为1，如果输入RGB彩色图ic为3，只有第一个卷积层如此计算，后面的卷积层都一样。
对于不同的框架，其卷积的输入每一个维度含意是不一样的，在数据预处理的时候要注意，不要大意，以免bug出错
如在keras中使用nchw，tf sess.run接口是nhwc

#### 注意事项

* global_step（日志记录）
  自动加一，在需要保存模型，写入日志，记录数值的地方，都需要设置global_stps，以免记录重复，因为我们没有手动的加每个日志的名，这里作用相当于时间戳。
* model.summary()
  查看模型细节
* resize与reshape
  resize会更改数据（元素个数可能改变），reshape只是扩充或减少维度（元素个数不变）
* tf.nn，tf.layes和tf.keras.layers的区别
  [参考链接](https://zhuanlan.zhihu.com/p/430121189)

### pipeline流水线需要构建的结构

#### 定义

模型（网络参数一般随机初始化，偏置有的时候随机但但更多的是0初始化）
损失函数
优化器：使用什么方法更新梯度

#### 训练（也可边训练边检测）

训练
更新梯度

#### 检测

精确度
其他尺度

#### 卷积

局部感受野和权值共享，与传统图像处理的区别，可学习的卷积核
大部分卷积神经网络都不需要偏置项，因为卷积核其本身表达能力是足够的

#### 反卷积

上采样 + 普通卷积

* 下采样：将数据尺寸缩小，卷积，池化，空洞卷积（增大感受野）
* 上采样，将数据尺寸扩大，插值，（强行resize，直接填充0）
* DropOut：训练时关闭掉一定比例的神经元（减少过拟合，防止梯度消失），预测时所有神经元都开启。
* 卷积层的输入为四维，全连接层输入为二维
* batch_size取决于当前服务器的显存
* epochs一般而言训练的epoch越多，效果越好，但是后期epoch性价比比较低
* global average pooling
  使用全局平均池化，代替全连接网络，GAP featrue_size一般等于channel数
* reshape函数的实质是什么，
* 增大感受野：空洞卷积，多个小卷积代替大卷积

#### VGG网络

* 核心用多个小卷积代替大卷积核，好处，减少参数量，让靠后的卷积核有更大的感受野，提取到更细节的特征。

#### Inception网络

* 核心：对一个隐层的输出，同时用不同大小的卷积核，提取不同尺度的特征，最后将他们concat起来，输入到下一层
* 使用多支损失函数，能够一定程度上保证梯度爆炸或梯度消失
* 采用两个一维卷积代替NxN的卷积，减少参数量，两个一维卷积的思路来自于传统图像处理的垂直核水平卷积核

#### ResNet网络

* 解决：解决了深度网络退化问题（网络越深，效果反而效果不如浅层网络）
* 核心：在主干网络上添加回路分支。residual connnection

#### Batch Normalization

BN，在channel维度上对数据转化到（0，1）的正态分布

#### FCN全卷积网络

* 核心：用卷积网络代替全连接网络，一般用（1，1）卷积核
* 优点1：权值共享，使权值学习的速度更快
* 优点2：保留了位置信息，使输出带有位置信息成为可能（经过全连接的数据无法还原出位置信息）
* 缺点：参数量少于全连接层学习效率不如全连接层

#### GAN

Generto生成网络的输入必须满足一定的分布，一般噪声满足高斯分布
一般先训练几轮判别器（Discriminator），再训练一次生成器

### 评价指标metrics

* accuracy
  
  [参考链接](https://blog.csdn.net/m0_37928549/article/details/85786628)

* #np.argmax() np.argmin() np.min() np.max()
  *其中axis=n就往哪个维度映射，返回数组的形状应该为去掉该维度的剩下维度组成的形状*
  [参考链接](https://blog.csdn.net/weixin_41560402/article/details/105277069)
  
  ```python
  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_, 1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  # 参数解释
  # np.argmax用法一样
  tf.argmax(x, axis=1) # 将每行的数据进行比较，取每行的最大值的下标，返回的大小为x的行大小
  tf.argmax(x, axis=0) # 将每列的数据进行比较，取每列的最大值的下标，返回的大小为x的列大小
  ```

tf.equal(x, y) # 返回一个bool列表，判断x，y对应元素的值是否相等，真为True，假为False
tf.cast(x, dtype=xx) # 转化数据类型
tf.reduce_mean() # 计算所有数的平均值

import tensorflow as tf
import numpy as np
sess = tf.Session()
m = sess.run(tf.truncated_normal((5,10,3), stddev = 0.1) )
print(m.shape)
print(np.argmax(m, 0).shape)
print(np.argmax(m, 1).shape)
print(np.argmax(m, 2).shape)

# output

(5, 10, 3)
(10, 3)
(5, 3)
(5, 10)

```

```

①任何信息是否都可转化某种特定的格式，然后再改格式进行比对，我忍为因该是可以的，因为目前无线信号的传播从某种意义上就完成了
②由《透明脑》的启发
③计算机技术使用层次：转发交流统计信息、硬件化控制设备、智能化控制设备、四者结合zh